{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "L4",
      "authorship_tag": "ABX9TyP4Q1USTHl72xEpg3PbWBRa",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "53739b95bb8448649c72706c48f3486d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f781babe5a4a43438b9526c4ccd7741a",
              "IPY_MODEL_e9036e137a834c0e8600bd526032a24b",
              "IPY_MODEL_330fb11b3b244e9c9390586a7ed08eb3"
            ],
            "layout": "IPY_MODEL_eb4bda610a9c4cdaaf2591d73a1c9817"
          }
        },
        "f781babe5a4a43438b9526c4ccd7741a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7ef129e74b214488a0f97145700b7000",
            "placeholder": "​",
            "style": "IPY_MODEL_de6907c6091b46e0bea82113a5e939bd",
            "value": "100%"
          }
        },
        "e9036e137a834c0e8600bd526032a24b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_380c4aa3d8b4491383e5638e946b4452",
            "max": 10,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b6ce385c8d9749c2bf4ae26b2fe5896e",
            "value": 10
          }
        },
        "330fb11b3b244e9c9390586a7ed08eb3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5f6011043afe486aa4609c1221f48531",
            "placeholder": "​",
            "style": "IPY_MODEL_658172f766cf4f52bbb8627c1e592b8f",
            "value": " 10/10 [1:39:28&lt;00:00, 597.13s/it]"
          }
        },
        "eb4bda610a9c4cdaaf2591d73a1c9817": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7ef129e74b214488a0f97145700b7000": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "de6907c6091b46e0bea82113a5e939bd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "380c4aa3d8b4491383e5638e946b4452": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b6ce385c8d9749c2bf4ae26b2fe5896e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5f6011043afe486aa4609c1221f48531": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "658172f766cf4f52bbb8627c1e592b8f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dmaresza/PyTorch-Course/blob/main/FoodVision_ViT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Notebook to create a Vision Transformer (ViT) feature extraction model, train it on the Food101 dataset, and deploy it to HuggingFace spaces using Gradio."
      ],
      "metadata": {
        "id": "01DVTGxv9SsF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup"
      ],
      "metadata": {
        "id": "IZmNkneM9jdx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "piIMiOrH8vOx",
        "outputId": "d386211c-0cd1-41d7-93ee-3a601af5c852"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] Couldn't find torchinfo... installing it.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2.4.0+cu121'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "# Getting necessary imports\n",
        "import torch\n",
        "import torchvision\n",
        "\n",
        "from torch import nn\n",
        "from torchvision import transforms\n",
        "\n",
        "try:\n",
        "  from torchinfo import summary\n",
        "except:\n",
        "  print(\"[INFO] Couldn't find torchinfo... installing it.\")\n",
        "  !pip install -q torchinfo\n",
        "  from torchinfo import summary\n",
        "\n",
        "# Check torch version\n",
        "torch.__version__"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# some Colab shell commands weren't working so had to add this bit\n",
        "import locale\n",
        "locale.getpreferredencoding = lambda: \"UTF-8\""
      ],
      "metadata": {
        "id": "P4UP_qx4CF7f"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Setting device globally\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "9haf8iu19nib",
        "outputId": "55d17a64-32eb-4854-9b45-ff639e44e8fa"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'cuda'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Instantiate model"
      ],
      "metadata": {
        "id": "UYuD9U9HB5qX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create ViT_B_16 pretrained weights, transforms and model\n",
        "vit_weights = torchvision.models.ViT_B_16_Weights.DEFAULT\n",
        "vit_transforms = vit_weights.transforms()\n",
        "vit_model = torchvision.models.vit_b_16(weights=vit_weights)\n",
        "\n",
        "# Freeze all of the base layers\n",
        "for param in vit_model.parameters():\n",
        "  param.requires_grad = False\n",
        "\n",
        "# Change classifier head to suit our needs\n",
        "vit_model.heads = nn.Sequential(\n",
        "    nn.Linear(in_features=768,\n",
        "              out_features=101))\n",
        "\n",
        "# Get model summary\n",
        "summary(model=vit_model,\n",
        "        input_size=(1, 3, 224, 224))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LjymmG5tB7Ok",
        "outputId": "398b6f5d-73b3-4b56-910e-f94a3901bede"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/vit_b_16-c867db91.pth\" to /root/.cache/torch/hub/checkpoints/vit_b_16-c867db91.pth\n",
            "100%|██████████| 330M/330M [00:01<00:00, 210MB/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "===============================================================================================\n",
              "Layer (type:depth-idx)                        Output Shape              Param #\n",
              "===============================================================================================\n",
              "VisionTransformer                             [1, 101]                  768\n",
              "├─Conv2d: 1-1                                 [1, 768, 14, 14]          (590,592)\n",
              "├─Encoder: 1-2                                [1, 197, 768]             151,296\n",
              "│    └─Dropout: 2-1                           [1, 197, 768]             --\n",
              "│    └─Sequential: 2-2                        [1, 197, 768]             --\n",
              "│    │    └─EncoderBlock: 3-1                 [1, 197, 768]             (7,087,872)\n",
              "│    │    └─EncoderBlock: 3-2                 [1, 197, 768]             (7,087,872)\n",
              "│    │    └─EncoderBlock: 3-3                 [1, 197, 768]             (7,087,872)\n",
              "│    │    └─EncoderBlock: 3-4                 [1, 197, 768]             (7,087,872)\n",
              "│    │    └─EncoderBlock: 3-5                 [1, 197, 768]             (7,087,872)\n",
              "│    │    └─EncoderBlock: 3-6                 [1, 197, 768]             (7,087,872)\n",
              "│    │    └─EncoderBlock: 3-7                 [1, 197, 768]             (7,087,872)\n",
              "│    │    └─EncoderBlock: 3-8                 [1, 197, 768]             (7,087,872)\n",
              "│    │    └─EncoderBlock: 3-9                 [1, 197, 768]             (7,087,872)\n",
              "│    │    └─EncoderBlock: 3-10                [1, 197, 768]             (7,087,872)\n",
              "│    │    └─EncoderBlock: 3-11                [1, 197, 768]             (7,087,872)\n",
              "│    │    └─EncoderBlock: 3-12                [1, 197, 768]             (7,087,872)\n",
              "│    └─LayerNorm: 2-3                         [1, 197, 768]             (1,536)\n",
              "├─Sequential: 1-3                             [1, 101]                  --\n",
              "│    └─Linear: 2-4                            [1, 101]                  77,669\n",
              "===============================================================================================\n",
              "Total params: 85,876,325\n",
              "Trainable params: 77,669\n",
              "Non-trainable params: 85,798,656\n",
              "Total mult-adds (M): 172.54\n",
              "===============================================================================================\n",
              "Input size (MB): 0.60\n",
              "Forward/backward pass size (MB): 104.09\n",
              "Params size (MB): 229.50\n",
              "Estimated Total Size (MB): 334.19\n",
              "==============================================================================================="
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Get dataset"
      ],
      "metadata": {
        "id": "bDTZKViH_Txv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import datasets\n",
        "from pathlib import Path\n",
        "\n",
        "# Set up data directory\n",
        "data_dir = Path(\"data\")\n",
        "\n",
        "# Create train dataset\n",
        "train_data = datasets.Food101(root=data_dir,\n",
        "                              split=\"train\",\n",
        "                              transform=vit_transforms,\n",
        "                              download=True)\n",
        "\n",
        "# Create test dataset\n",
        "test_data = datasets.Food101(root=data_dir,\n",
        "                             split=\"test\",\n",
        "                             transform=vit_transforms,\n",
        "                             download=True)\n",
        "\n",
        "len(train_data), len(test_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dIdae6ksAAWi",
        "outputId": "658e4d76-68c2-453e-c8ae-466ace1cce78"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://data.vision.ee.ethz.ch/cvl/food-101.tar.gz to data/food-101.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4996278331/4996278331 [03:50<00:00, 21686204.29it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/food-101.tar.gz to data\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(75750, 25250)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Large batch size to make better use of GPU\n",
        "BATCH_SIZE = 512\n",
        "NUM_WORKERS = os.cpu_count()\n",
        "\n",
        "train_dataloader = torch.utils.data.DataLoader(dataset=train_data,\n",
        "                                               batch_size=BATCH_SIZE,\n",
        "                                               shuffle=True,\n",
        "                                               num_workers=NUM_WORKERS)\n",
        "\n",
        "test_dataloader = torch.utils.data.DataLoader(dataset=test_data,\n",
        "                                              batch_size=BATCH_SIZE,\n",
        "                                              shuffle=False,\n",
        "                                              num_workers=NUM_WORKERS)\n",
        "\n",
        "len(train_dataloader), len(test_dataloader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AcYjLFMyErps",
        "outputId": "d6738ff8-2636-438e-e555-522d882637cc"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(148, 50)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train"
      ],
      "metadata": {
        "id": "s5TAuSK6HrPs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up optimizer\n",
        "optimizer = torch.optim.Adam(params=vit_model.parameters())\n",
        "\n",
        "# Set up loss function\n",
        "loss_fn = torch.nn.CrossEntropyLoss(label_smoothing=0.1)"
      ],
      "metadata": {
        "id": "gjXpShh3V1Hd"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm.auto import tqdm\n",
        "\n",
        "# 10 epochs to give model sufficient time to train, but not take TOO long\n",
        "EPOCHS = 10\n",
        "\n",
        "vit_model.to(device)\n",
        "\n",
        "for epoch in tqdm(range(EPOCHS)):\n",
        "  ### TRAINING\n",
        "  vit_model.train()\n",
        "\n",
        "  train_loss, train_acc = 0, 0\n",
        "\n",
        "  for batch, (X, y) in enumerate(train_dataloader):\n",
        "    # Forward pass\n",
        "    X, y = X.to(device), y.to(device)\n",
        "    y_pred = vit_model(X)\n",
        "\n",
        "    # Calculate & accumulate loss\n",
        "    loss = loss_fn(y_pred, y)\n",
        "    train_loss += loss.item()\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    loss.backward()\n",
        "\n",
        "    optimizer.step()\n",
        "\n",
        "    # Calculate & accumulate accuracy\n",
        "    y_pred_class = torch.argmax(torch.softmax(y_pred, dim=1), dim=1)\n",
        "    train_acc += (y_pred_class == y).sum().item()/len(y_pred)\n",
        "\n",
        "  # Get average train loss & average train accuracy for 1 epoch\n",
        "  train_loss /= len(train_dataloader)\n",
        "  train_acc /= len(train_dataloader)\n",
        "\n",
        "  ### TESTING\n",
        "  vit_model.eval()\n",
        "\n",
        "  test_loss, test_acc = 0, 0\n",
        "\n",
        "  with torch.inference_mode():\n",
        "    for batch, (X, y) in enumerate(test_dataloader):\n",
        "      X, y = X.to(device), y.to(device)\n",
        "      test_pred_logits = vit_model(X)\n",
        "\n",
        "      # Calculate & accumulate loss\n",
        "      loss = loss_fn(test_pred_logits, y)\n",
        "      test_loss += loss.item()\n",
        "\n",
        "      # Calculate & accumulate accuracy\n",
        "      test_pred_labels = test_pred_logits.argmax(dim=1)\n",
        "      test_acc += (test_pred_labels == y).sum().item()/len(test_pred_labels)\n",
        "\n",
        "  # Get average test loss & average test accuracy for 1 epoch\n",
        "  test_loss /= len(test_dataloader)\n",
        "  test_acc /= len(test_dataloader)\n",
        "\n",
        "  # Print out results for each epoch\n",
        "  print(\n",
        "      f\"Epoch: {epoch + 1} | \"\n",
        "      f\"train_loss: {train_loss:.4f} | \"\n",
        "      f\"train_acc: {train_acc:.4f} | \"\n",
        "      f\"test_loss: {test_loss:.4f} | \"\n",
        "      f\"test_acc: {test_acc:.4f}\"\n",
        "  )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 223,
          "referenced_widgets": [
            "53739b95bb8448649c72706c48f3486d",
            "f781babe5a4a43438b9526c4ccd7741a",
            "e9036e137a834c0e8600bd526032a24b",
            "330fb11b3b244e9c9390586a7ed08eb3",
            "eb4bda610a9c4cdaaf2591d73a1c9817",
            "7ef129e74b214488a0f97145700b7000",
            "de6907c6091b46e0bea82113a5e939bd",
            "380c4aa3d8b4491383e5638e946b4452",
            "b6ce385c8d9749c2bf4ae26b2fe5896e",
            "5f6011043afe486aa4609c1221f48531",
            "658172f766cf4f52bbb8627c1e592b8f"
          ]
        },
        "id": "-2BGwOwTIz0c",
        "outputId": "c704a551-8de1-4fbd-eda0-e2b7825d3f87"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/10 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "53739b95bb8448649c72706c48f3486d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1 | train_loss: 2.5963 | train_acc: 0.5122 | test_loss: 1.9517 | test_acc: 0.6732\n",
            "Epoch: 2 | train_loss: 1.9912 | train_acc: 0.6638 | test_loss: 1.7974 | test_acc: 0.7219\n",
            "Epoch: 3 | train_loss: 1.8673 | train_acc: 0.7021 | test_loss: 1.7328 | test_acc: 0.7411\n",
            "Epoch: 4 | train_loss: 1.7953 | train_acc: 0.7271 | test_loss: 1.6944 | test_acc: 0.7516\n",
            "Epoch: 5 | train_loss: 1.7476 | train_acc: 0.7419 | test_loss: 1.6703 | test_acc: 0.7591\n",
            "Epoch: 6 | train_loss: 1.7116 | train_acc: 0.7529 | test_loss: 1.6557 | test_acc: 0.7615\n",
            "Epoch: 7 | train_loss: 1.6847 | train_acc: 0.7616 | test_loss: 1.6444 | test_acc: 0.7668\n",
            "Epoch: 8 | train_loss: 1.6616 | train_acc: 0.7707 | test_loss: 1.6369 | test_acc: 0.7683\n",
            "Epoch: 9 | train_loss: 1.6426 | train_acc: 0.7770 | test_loss: 1.6317 | test_acc: 0.7703\n",
            "Epoch: 10 | train_loss: 1.6268 | train_acc: 0.7825 | test_loss: 1.6254 | test_acc: 0.7722\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Save the model"
      ],
      "metadata": {
        "id": "28PAHE-5UuzP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_path = Path(\"models\")\n",
        "model_path.mkdir(parents=True,\n",
        "                 exist_ok=True)\n",
        "\n",
        "model_name = \"pretrained_vit_feature_extractor_food101.pth\"\n",
        "model_save_path = model_path / model_name\n",
        "\n",
        "torch.save(obj=vit_model.state_dict(),\n",
        "           f=model_save_path)"
      ],
      "metadata": {
        "id": "TRxyXK8PUwHI"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Build Gradio App"
      ],
      "metadata": {
        "id": "ClF9tqruWNfa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "\n",
        "# Create directory for demo files\n",
        "demo_path = Path(\"demos/food101/\")\n",
        "\n",
        "if demo_path.exists():\n",
        "  shutil.rmtree(demo_path)\n",
        "  demo_path.mkdir(parents=True, exist_ok=True)\n",
        "else:\n",
        "  demo_path.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "!ls demos/food101/"
      ],
      "metadata": {
        "id": "03vnSHMaWUw6"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Add a few example images\n",
        "demo_examples_path = demo_path / \"examples\"\n",
        "demo_examples_path.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "examples = [Path('data/food-101/images/cannoli/2034686.jpg'),\n",
        "            Path('data/food-101/images/guacamole/36147.jpg'),\n",
        "            Path('data/food-101/images/steak/1053665.jpg')]\n",
        "\n",
        "for example in examples:\n",
        "  destination = demo_examples_path / example.name\n",
        "  print(f\"[INFO] Copying {example} to {destination}\")\n",
        "  shutil.copy2(src=example, dst=destination)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "44q8cA7X7wVm",
        "outputId": "07b7f57f-8c2a-493a-d255-18192bca6429"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] Copying data/food-101/images/cannoli/2034686.jpg to demos/food101/examples/2034686.jpg\n",
            "[INFO] Copying data/food-101/images/guacamole/36147.jpg to demos/food101/examples/36147.jpg\n",
            "[INFO] Copying data/food-101/images/steak/1053665.jpg to demos/food101/examples/1053665.jpg\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class_names = train_data.classes\n",
        "\n",
        "# Create path to Food101 class names\n",
        "class_names_path = demo_path / \"class_names.txt\"\n",
        "\n",
        "# Write Food101 class names to text file\n",
        "with open(class_names_path, \"w\") as f:\n",
        "  print(f\"[INFO] Saving Food101 class names to {class_names_path}\")\n",
        "  f.write(\"\\n\".join(class_names)) # new line per class name"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tz6br8qI9CMx",
        "outputId": "99654e1e-d279-4d5f-8ed9-df4bc6c80121"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] Saving Food101 class names to demos/food101/class_names.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile demos/food101/model.py\n",
        "# File for instantiating ViT model\n",
        "import torch\n",
        "import torchvision\n",
        "from torch import nn\n",
        "\n",
        "def create_vit_model(num_classes:int=3,\n",
        "                     seed:int=42):\n",
        "  # Create ViT_B_16 pretrained weights, transforms and model\n",
        "  weights = torchvision.models.ViT_B_16_Weights.DEFAULT\n",
        "  transforms = weights.transforms()\n",
        "  model = torchvision.models.vit_b_16(weights=weights)\n",
        "\n",
        "  # Freeze all of the base layers\n",
        "  for param in model.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "  # Change classifier head to suit our needs\n",
        "  model.heads = nn.Sequential(\n",
        "      nn.Linear(in_features=768,\n",
        "                out_features=num_classes)\n",
        "  )\n",
        "\n",
        "  return model, transforms"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hKMVFelB8aY2",
        "outputId": "b0607c0d-3168-487f-f420-f60e17c5b044"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing demos/food101/model.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Move saved model into demo folder\n",
        "!mv models/pretrained_vit_feature_extractor_food101.pth demos/food101/"
      ],
      "metadata": {
        "id": "Hx2ii6sfl1GO"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile demos/food101/app.py\n",
        "# File to build Gradio application\n",
        "### 1. Imports and class names setup ###\n",
        "import gradio as gr\n",
        "import os\n",
        "import torch\n",
        "\n",
        "from model import create_vit_model\n",
        "from timeit import default_timer as timer\n",
        "from typing import Tuple, Dict\n",
        "\n",
        "# Set up class names\n",
        "with open(\"class_names.txt\", \"r\") as f:\n",
        "  class_names = [food_name.strip() for food_name in f.readlines()]\n",
        "\n",
        "### 2. Model and transforms preparation ###\n",
        "# Create model and transforms\n",
        "vit, vit_transforms = create_vit_model(\n",
        "    num_classes=len(class_names))\n",
        "\n",
        "# Load saved weights\n",
        "vit.load_state_dict(\n",
        "    torch.load(f=\"pretrained_vit_feature_extractor_food101.pth\",\n",
        "               map_location=torch.device(\"cpu\")) # load the model to the CPU\n",
        ")\n",
        "\n",
        "### 3. Predict function ###\n",
        "def predict(img) -> Tuple[Dict, float]:\n",
        "  # Start a timer\n",
        "  start_time = timer()\n",
        "\n",
        "  # Transform the input image for use with ViT\n",
        "  img = vit_transforms(img).unsqueeze(0) # unsqueeze = add batch dimension on 0th index\n",
        "\n",
        "  # Put model inot eval mode, make prediction\n",
        "  vit.eval()\n",
        "  with torch.inference_mode():\n",
        "    # Pass transformed image through the model and turn the prediction logits into probabilities\n",
        "    pred_probs = torch.softmax(vit(img), dim=1)\n",
        "\n",
        "  # Create a prediction label and prediction probability dictionary\n",
        "  pred_labels_and_probs = {class_names[i]: float(pred_probs[0][i]) for i in range(len(class_names))}\n",
        "\n",
        "  # Calculate pred time\n",
        "  end_time = timer()\n",
        "  pred_time = round(end_time - start_time, 4)\n",
        "\n",
        "  # Return pred dict and pred time\n",
        "  return pred_labels_and_probs, pred_time\n",
        "\n",
        "### 4. Gradio app ###\n",
        "\n",
        "# Create title, description, and article\n",
        "title = \"FoodVision BIG 🍔👁\"\n",
        "description = \"A [Vision Transformer (ViT)](https://pytorch.org/vision/stable/models/generated/torchvision.models.vit_b_16.html#torchvision.models.vit_b_16) feature extractor computer vision model to classify images of 101 classes of food from the [Food101 dataset](https://www.kaggle.com/datasets/dansbecker/food-101/data).\"\n",
        "article = \"Created at [FoodVision](https://github.com/dmaresza/PyTorch-Course/blob/main/FoodVision_ViT.ipynb).\"\n",
        "\n",
        "# Create example list\n",
        "example_list = [[\"examples/\" + example] for example in os.listdir(\"examples\")]\n",
        "\n",
        "# Create the Gradio demo\n",
        "demo = gr.Interface(fn=predict, # maps inputs to outputs\n",
        "                    inputs=gr.Image(type=\"pil\"),\n",
        "                    outputs=[gr.Label(num_top_classes=5, label=\"Predictions\"),\n",
        "                             gr.Number(label=\"Prediction time (s)\")],\n",
        "                    examples=example_list,\n",
        "                    title=title,\n",
        "                    description=description,\n",
        "                    article=article)\n",
        "\n",
        "# Launch the demo\n",
        "demo.launch(debug=False)"
      ],
      "metadata": {
        "id": "TyWLwBVy_qk-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5c163db2-a57f-4240-c6fd-7c4037eb939a"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing demos/food101/app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile demos/food101/requirements.txt\n",
        "torch==2.4.0\n",
        "torchvision==0.19.0\n",
        "gradio==4.42.0"
      ],
      "metadata": {
        "id": "cKyQ5K_i8lAb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c64c709a-843f-4031-fdb5-d06bfdc813f7"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing demos/food101/requirements.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Change into the food101 directory and then zip it from the inside\n",
        "!cd demos/food101 && zip -r ../foodvision.zip * -x \"*.pyc\" \"*.ipynb\" \"*__pycache__*\" \"*ipynb_checkpoints*\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yqLcLu7WnGzl",
        "outputId": "649422a2-7b93-41d6-c124-b54f6aa585e5"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  adding: app.py (deflated 54%)\n",
            "  adding: class_names.txt (deflated 48%)\n",
            "  adding: examples/ (stored 0%)\n",
            "  adding: examples/36147.jpg (deflated 1%)\n",
            "  adding: examples/2034686.jpg (deflated 1%)\n",
            "  adding: examples/1053665.jpg (deflated 1%)\n",
            "  adding: model.py (deflated 47%)\n",
            "  adding: pretrained_vit_feature_extractor_food101.pth (deflated 7%)\n",
            "  adding: requirements.txt (deflated 6%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download\n",
        "try:\n",
        "  from google.colab import files\n",
        "  files.download(\"demos/foodvision.zip\")\n",
        "except:\n",
        "  print(f\"Not running in Google Colab, can't use google.colab.files.download(), please download foodvision.zip manually.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "b2ePYO-bnTOR",
        "outputId": "1319654d-e85c-42cf-d880-cfd10c628d1b"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_1b781ab6-9281-43ec-aaf7-9e07b53e8eab\", \"foodvision.zip\", 319607300)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2IZrN-LfnecA"
      },
      "execution_count": 18,
      "outputs": []
    }
  ]
}